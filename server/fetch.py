# -*- coding: utf-8 -*-
# ^^^ ÙŠØ¬Ø¨ Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ù„ØªØ¬Ù†Ø¨ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªØ±Ù…ÙŠØ² (SyntaxError) Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø¹Ø§Ù„Ù…ÙŠØ©.

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import requests
import time
import sys
import urllib.parse 
import random # Ù„Ø¥Ø¶Ø§ÙØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¹Ù„Ù‰ User-Agent Ùˆ ÙØªØ±Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±

# ------------------------------------------------------------------
# Â Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¬Ù„Ø¨ HTML (Selenium Ù„Ù€ RedbubbleØŒ Requests Ù„Ù€ Teepublic)
# ------------------------------------------------------------------

def get_html_with_selenium(url):
    """ÙŠØ³ØªØ®Ø¯Ù… Selenium Ù„Ø¬Ù„Ø¨ HTML Ø¨Ø¹Ø¯ ØªÙ†ÙÙŠØ° JavaScript (Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€ Redbubble)."""
    driver = None
    try:
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Chrome (Headless)
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        # ÙˆÙƒÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ø­Ø¯ÙŠØ«
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        print(f"INFO: Fetching with Selenium for URL: {url}", file=sys.stderr)
        driver.get(url)
        
        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ (2-6 Ø«ÙˆØ§Ù†Ù) Ù„Ø¶Ù…Ø§Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆÙ…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¥Ù†Ø³Ø§Ù†
        time.sleep(random.uniform(2, 6)) 
        
        html_content = driver.page_source
        return html_content
    except Exception as e:
        print(f"ERROR: Selenium failed for {url}. Reason: {e}", file=sys.stderr)
        return None
    finally:
        if driver:
            driver.quit()

def get_html_with_requests(url, params):
    """ÙŠØ³ØªØ®Ø¯Ù… Requests Ù„Ø¬Ù„Ø¨ HTML (Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€ Teepublic Ø£Ùˆ Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ Ù„Ø§ ÙŠØ­ØªØ§Ø¬ JS)."""
    # ÙˆÙƒÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø£ÙƒØ«Ø± Ù„Ù€ Requests
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15',
        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:128.0) Gecko/20100101 Firefox/128.0'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8',
        'Connection': 'keep-alive',
    }
    
    try:
        response = requests.get(url, params=params, headers=headers)
        if response.status_code == 200:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ø³ØªÙ†ØªØ¬ Ù„Ø¶Ù…Ø§Ù† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
            response.encoding = response.apparent_encoding 
            return response.text
        else:
            print(f"WARNING: Requests failed. Status code: {response.status_code}. URL: {response.url}", file=sys.stderr)
            return None
    except requests.exceptions.RequestException as e:
        print(f"ERROR: Requests exception occurred: {e}", file=sys.stderr)
        return None

# ------------------------------------------------------------------
# Â Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ³ØªØ¯Ø¹ÙŠ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
# ------------------------------------------------------------------

def fetch_html_for_site(input_name, start_page, end_page, site_name):
    site_name = site_name.lower()
    all_html_content = ""
    
    # ğŸ’¥ Ø§Ù„Ø­Ù„ 2: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ (stdout) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ±Ù…ÙŠØ² UTF-8
    # Ù‡Ø°Ø§ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© UnicodeEncodeError Ø¹Ù†Ø¯ Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    try:
        if sys.stdout.encoding.lower() != 'utf-8':
             sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        # ØªØ¬Ø§ÙˆØ² ÙÙŠ Ø­Ø§Ù„ Ø¹Ø¯Ù… Ø¯Ø¹Ù… reconfigure ÙÙŠ Ø¥ØµØ¯Ø§Ø± Ø¨Ø§ÙŠØ«ÙˆÙ† Ù‚Ø¯ÙŠÙ…
        pass 
    
    if site_name not in ['redbubble', 'teepublic']:
        print(f"Error: Unsupported site name '{site_name}'. Must be 'teepublic' or 'redbubble'.", file=sys.stderr)
        sys.exit(1)

    print(f"Starting to fetch HTML for '{input_name}' from {site_name} (Pages {start_page} to {end_page})...", file=sys.stderr)
    
    for page_number in range(start_page, end_page + 1):
        html_content = None
        
        # 1. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹
        if site_name == 'redbubble':
            encoded_query = urllib.parse.quote_plus(input_name)
            # Redbubble URL Structure (Page number is a query param)
            url_to_fetch = f"https://www.redbubble.com/fr/shop/{encoded_query}"
            params = {
                'country': 'TN',
                'iaCode': 'all-stickers',
                'locale': 'fr',
                'pageSize': '82',
                'sortOrder': 'trending',
                'stickerFinish': 'stickerFinish-matte',
                'page': page_number 
            }
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Selenium
            # Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª ÙÙŠ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ø£Ù† Selenium ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            full_url = url_to_fetch + '?' + urllib.parse.urlencode(params)
            html_content = get_html_with_selenium(full_url)
            
        elif site_name == 'teepublic':
            # Teepublic URL Structure (Page number and query are query params)
            url_to_fetch = "https://www.teepublic.com/stickers"
            params = {
                'canvas_subclass': 'sticker',
                'sort': 'popular',
                'page': page_number,
                'query': input_name
            }
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Requests
            html_content = get_html_with_requests(url_to_fetch, params)
        
        # 2. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if html_content:
            all_html_content += f"--- HTML Content for Page {page_number} from {site_name} ---\n"
            all_html_content += html_content
            all_html_content += "\n\n"
            print(f"Successfully fetched HTML for page {page_number} from {site_name}.", file=sys.stderr)
        else:
            print(f"Failed to fetch content for page {page_number} from {site_name}. Skipping.", file=sys.stderr)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        time.sleep(random.uniform(1, 4)) 

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ù„Ù‰ stdout Ù„ÙŠØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© Node.js
    print(all_html_content)

if __name__ == "__main__":
    if len(sys.argv) < 5:
        print("Usage: python scraper_script.py <search_query> <start_page> <end_page> <site_name>", file=sys.stderr)
        sys.exit(1)
        
    search_query = sys.argv[1]
    
    try:
        start_page = int(sys.argv[2])
        end_page = int(sys.argv[3])
    except ValueError:
        print("Error: Page numbers must be integers.", file=sys.stderr)
        sys.exit(1)
        
    site_name = sys.argv[4]

    fetch_html_for_site(search_query, start_page, end_page, site_name)